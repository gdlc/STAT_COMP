---
title: "Methods for High Dimensional Regression"
author: "G. de los Campos"
date: "11/29/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,eval=TRUE)
```

Up to now, we considered methods that estimate parameters by either maximizing the likelihood function (ML) or by minimaizing the residual sum of squares (OLS). These methods have reasonably good statistical properties (e.g., OLS is unbiased and has minimum variance among the class of linear unbiased estiamtors, ML is asymptotically unbiased and asymptotically efficient). However, the performace of these methods can be sub-optimal when the number of parameters to be estimated (e.g., the number of regression coefficients) is large relative to sample size.

In this last module of the course we will consider methods that are tailored for problems involving a large number of predictors. We will cover the following approaches: 

 - **1) Independent screening**: This approach selects predictors using a marginal association test (i.e., testing the association of the response and each predictor, one predictor at a time) and builds models using the top-q (*q*=1,2,...). An optimal model DF can be chosen by evaluating thea ability of the model to predict testing data for models with 1DF, 2DF,...
 - **2) Forward Regression**: This approach builds a sequence of models, starting from a null model (e.g., intercept only), adding one predictor at a time, each time adding to the model the predictor that produces the largest reduction in the RSS (alternatives to this that we won't discuss are backgward elimination and methods that combine forward and backward methods).
 - **3) Penalized Regressions**: This approach estimates effects using a penalized sum of squares. We will consider three methods:  Ridge Regression, Lasso, and Elastic Net. The extent of regularization will be controled by a parameter ($\lambda$) which will be chosen to maximize prediction accuracy in testing data. We will also discuss best subset selection briefly.
 - **4) Bayesian Srhinkage and Variable Selection methods**: In Bayesian regression the choice of prior will determine whether the model performs shrinkage, variable selection, or a combination of the two. We will present examples using shrinkage and variable selection priors.
 

**Data**

To illustrate the application of the methods listed above, we will use a data set available in the [BGLR](https://cran.r-project.org/web/packages/BGLR/index.html) R-package. This data set provides four phenotypes (see object `wheat.Y`) for 599 wheat lines that were genotyped at 1,279 genetic markers (see object `wheat.X`).

**Loading the data**

This code below loads the data, center, and scales the the genotypes. While centering and scaling is not strictly needed, it is often a good practice when using penalized (e.g., Lasso) or Bayesian regressions.

```{r}
  library(BGLR)
  data(wheat)
  head(wheat.Y)
  dim(wheat.X)
  
  X=scale(wheat.X,center=TRUE,scale=TRUE)
  y=wheat.Y[,2] # picks one phenotype
  
  N<-nrow(X) ; p<-ncol(X)
```

We will compare models based on their ability to predict data that was not used to fit the models. The following code produces a training-testing partition that we will use for all mehtods.

**Creating a Training-Testing partition**

```{r}
 set.seed(12345)
 tst<-sample(1:N,size=150,replace=FALSE)
 XTRN<-X[-tst,]
 yTRN<-y[-tst]
 XTST<-X[tst,]
 yTST<-y[tst]
  
```


### 1) Indenpendent screening

To implement this approach, we will first rank predictors based on the marginal association of each predictor with the response. This is done using the training data only.

```{r}
 pValues<-numeric()
 for(i in 1:p){
	fm<-lsfit(y=yTRN,x=XTRN[,i])
	pValues[i]<-ls.print(fm,print.it=F)$coef[[1]][2,4] # extracts p-value, similar to lm() but a bit faster
 }
```

```{r, echo=FALSE,fig.cap="Marginal association p-value", out.width = '85%'}
 plot(-log10(pValues),cex=.5,col=2)
```


Let's now build models using the top-*q* (*q*=1,....,300) markers. The script:
 - Ranks markers based on p-values (from smallest to largest).
 - Fit models using the top 1, top 2, ..., top-q markers using data from the training set.
 - For each of the fitted model the script computes the correlation between phenotype and predictions within the training data and in the testing data.

**Building prediction models using the top-q markers**

```{r}

 mrk_rank<-order(pValues); corTRN<-numeric(); corTST<-numeric()
 for(i in 1:300){	
	tmpIndex<- mrk_rank[1:i]
	ZTRN=XTRN[,tmpIndex,drop=F]
	ZTST=XTST[,tmpIndex,drop=F]
	
	fm<-lm(yTRN~ZTRN)
	bHat=coef(fm)[-1]
	bHat<-ifelse(is.na(bHat),0,bHat)
	
	yHatTRN=ZTRN%*%bHat
  corTRN[i]<-cor(yTRN,yHatTRN)
  
	yHatTST=ZTST%*%bHat
	corTST[i]<-cor(yTST,yHatTST)
	
 }
```

```{r, echo=FALSE,fig.cap="Correlation between predictions and phenotypes in training and testing set by model DF.", out.width = '85%'}
 plot(c(0,corTRN),x=0:length(corTRN),type='o',col=2,ylab='Correlation-Training',
       xlab='Number of markers',ylim=c(0,.9))
 
 lines(x=0:length(corTST),y=c(0,corTST),col=4)
 points(x=0:length(corTST),y=c(0,corTST),col=4)
 abline(v=which.max(corTST),lty=2)
 abline(h=corTST[which.max(corTST)],lty=2)
```

**Remarks**

  - Goodness of fit in the training data set (`corTRN`, blue) increases with DF
  - However prediction accuracy in testing data (`corTST`, red) increases, reaches a plateau, and then decreases.

\newpage
The curves presented in the previous figure are estimates subject to sampling variability. To quantify this and to reduce the variance of estimtes we can conduct many training-testing partitions and average across them. This is illustrated in the following figure, each of the skyblue lines is an estiamte derived from a training-testing partition. The solid red line is an averag across partitions.

```{r , echo=FALSE, fig.cap="Correlation between predictions and phenotypes in testing data by model DF. (100 training-testing partitions)", out.width = '85%'}
 knitr::include_graphics("~/Dropbox/STAT_COMP/2020/VarScreening.pdf")
```
  
\newpage

### 2) Forward Regression


One limitation of independent screening is that the predictors' rank is based on the marginal association with the outcome. This does not guarantee that in each step the predictor added to the model is the one that produces the best improvement to the model. To see this, imagine a situation where two predictors (say X1 and X2) are almost perfectly correlated. If they are almost perfectly correlated, they will receive a similar rank based on their marginal association with the outcome. Imagine X2 was ranked before X1, and that you are at a step where X2 was already included in the model. Would the inclusion of X1 in the model, after X2 was added, improve the model's ability to fit the training data? The answer is not because X1 is almost perfectly correlated with another variable (X1) already in the model; therefore, X1 provides very little additional information. 

The problem above-described suggests that perhaps we should evaluate predictors based on their potential contribution to the model, conditional on all the predictors that have already ented into the model. This is what forward regression does. Schematically:

   1) We assess the marginal association between the outcome and each predictor (say we have p candidate predictors, X1, ..., Xp.
   2) We include the top-ranked predictor, assuming it is X2 the model becomes $y_i=\mu+x_{2i}\beta_2+\varepsilon_i$.
   3) Subsequently, we evaluate the all possible models with two predictors: $y_i=\mu+x_{2i}\beta_2+x_{3i}\beta_j+\varepsilon_i$ for $j=1,3,...,p$. We chose among these models the one with the smallest residual sum of squares.
   4) We repeat #3, each time adding to the current model the one with the smallest RSS.
   

The base package of R includes the `step()` function which can be used for forward regression, backward elimination, and stage-wise approaches that combine forward and backward regression. These functions can be used with objects from multiple regression methods (e.g., lm, glm). They perform very well for problems involving a limited number of candidate predictors. However, these functions can be very slow if the number of predictors is large. The `BGData` provides a function (FWD) for forward regression that is optimized for big data with large number of predictors. The following code uses this function and compares the results with that of independent screening.  

```{r fig.cap="Prediction correlations obtained with forward regression.", out.width = '85%'}

 library(BGDataExt)

 FM=FWD(y=yTRN,X=XTRN,df=100,verbose=FALSE)
 
 FM$path$variable # gives the order with which predictors entered the model
 #FM$path$RSS, $path$AIC, $path$BIC, $path$LogLik give the corresponding statistics at each step in the forward path
 
 dim(FM$B) # gives the estimated effects at each step in the forward path
 
 
 
 COR=rep(NA,100)
 
 for(i in 2:101){ # first model is the intercept-only
   COR[i-1]=cor(yTST,cbind(1,XTST)%*%FM$B[,i])
 }
 
 plot(COR,x=1:100,type='o')
```

We see that with ~60 predictors, the forward regression achieves a slighlty higher prediction correlation than any of the correlations obtained with indpeendent screening.


### Best subset selection

The problem we are trying to tackle can be described as follows:

  - We have an outcome (Y) and a large number of predictors (X1,...,Xp).
  - We want to find the best model among all the models that can be formed using these $p$ predictors.
  
The problem is known as *best subset selection*. One difficulty is that the number of possible models can be extrremely large; thus, evaluating all possible models is usually not doable. Independent screening, forward regression (and related procedures such as backward elimination) can be seen as approaches that evaluate a subset of all possible models (the subest is defined by the path produced by these algorithms). While none of these procedures guarantee that we can achieve the best model, the hope is that the path develped by these algorithms will include a model with a perfromance (e.g., prediction accuracy) close to the best possible model. It turns out that best subset selection can be seen as a penalized regression problem--a approach that we discuss below.

### Regularized Regression

A groundbreaking study by [James & Stein (1961)](https://projecteuclid.org/euclid.bsmsp/1200512173) showed that in some settings the ML estimator could be indmisible; that is, they showed that in some cirumbstances there was another estimator that had lower Mean-Squared Error (MSE) over the entire parameter space. Their estimator shrunks the least square estimates towards zero; thus reducing the variance of estimates. 

Recall that the MSE of an estimator can be decomposed as the sum of the variance plus the squared of the bias of the estimator $MSE=E[(\hat{\theta}-\theta)^2]=Variance+Bias^2$. Shrinkage reduces the variance of estimates at the expense of some bias. However, when the number of parameters to be estimated is large, the reduction in variance overcomes the increase in bias; thus leding to a reduction in MSE.

There are many ways to obtain regularized estimates; two commonly used approaches are penalized and Bayesian methods. We discussed each of them in the next two sections. In most cases there is a duality between the two approaches by which a penalized estimator can be viewed as the posterior mode from a Bayesian model.

### 3) Penalized regressions using glmnet

In a penalized regression, estimates are obtained by minimizing a penalized log-likelihood or, in the case of linear models, a penalized residual sum of squares:


$\hat{\beta}=argmin \{(y-X\beta)'(y-X\beta)+\lambda J(\beta) \}$

where $J(\beta)$ is a penalty function. Common choices for the penalty function are the

  - L2-norm, $J(\beta)=\sum_{j}{\beta_j^2}$ (aka Ridge Regression, [Hoerl and Kennard 1970](https://www.jstor.org/stable/1271436?seq=1) ), 
  - L-1 norm $J(\beta)=\sum_{j}{|\beta_j|}$ (aka Lasso, [Tibshirani, 1996](https://www.jstor.org/stable/2346178?seq=1) ), and, 
  - Linear combinations of the two $J(\beta)=(1-\alpha)\sum_{j}{\beta_j^2}+\alpha\sum_{j}{|\beta_j|}$ (aka [Elastic Net, Zhou and Hastie, 2005](https://www.jstor.org/stable/3647580?seq=1) ), for some $\alpha \in [0,1]$. 

Choosing $\lambda=0$ leads Ordinary Least Squares estimates. Ridge regression shrunk OLS estimates towards zero, without making variable selection. Lasso and Elastic Net combine variable selection and shrinkage.


Commonly, these models are fitted over a grid of values of the regularization parameter ($\lambda$); an optimal value for that parameter is often chosen by evaluating the ability of the fitted models to predict data that was not used to train the models (i.e., testing data).

**Note:** Best subset selection is obtained using as a penality a function that counts the number of non-zero effects: $J(\beta)=\Sigma_j{1(\beta_j \neq 0)})$.

#### Ridge Regression (RR)

In the RR ([Hoerl and Kennard 1970](https://www.jstor.org/stable/1271436?seq=1) ) $J(\beta)=\sum_{j}{\beta_j^2}=\beta'\beta$; thus, the objective function becomes


$\hat{\beta}=argmin \{(y-X\beta)'(y-X\beta)+\lambda \beta'\beta \}=argmin \{y'y +\beta'(X'X+I\lambda)\beta-2\beta'X'y\}$

The solution can be shown to be

$\hat{\beta}=(X'X+I\lambda)^{-1}X'y$

Adding $\lambda$ to the diagonal entries of $X'X$ srhinks estimates towards zero. This is illustrated in the following simplified example.

```{r}
  set.seed(195021)
  # Toy simulation
   n=50 
   p=3  # number of predictors
   W=matrix(nrow=n,ncol=p,rnorm(n*p))
   b=c(-1,1,2) # true effects
   signal=W%*%b
   error=rnorm(sd=sd(signal),n=length(signal))
   y=signal+error
   # centering to avoid the need of including an intercept
   W=scale(W,center=T,scale=F)
   y=y-mean(y)
   
  # OLS
  WW=crossprod(W)
  Wy=crossprod(W,y)
  bOLS=solve(WW,Wy)
  
  # Ridge regression
  lambda=3
  C=WW
  diag(C)=diag(C)+lambda
  bRR_3=solve(C,Wy)
  
  lambda=10
  C=WW
  diag(C)=diag(C)+lambda
  bRR_10=solve(C,Wy)
  
  
  lambda=100
  C=WW
  diag(C)=diag(C)+lambda
  bRR_100=solve(C,Wy)
  round(cbind('true_effect'=b,'ols'=bOLS,'RR_3'=bRR_3,'RR_10'=bRR_10,'RR_100'=bRR_100),3)
```

Let's compare estimates in just one Monte Carlo (MC) replicate (to estimate MSE we should average over many MC replicates).

```{r}
 sum((b-bOLS)^2)
 sum((b-bRR_3)^2)
 sum((b-bRR_10)^2)
 sum((b-bRR_100)^2)
```

In this example suign $\lambda=3$ or $\lambda=10$ improved the estimates (smaller distnace to the true parameter values), but using $\lambda=100$ induced too much shrinkage towards zero, thus increasing the squared-difference between esitmates and true parameter values.

Unlike the Ridge Regression, Lasso and Elastic Net estimates do not have a closed form; however, estimates can be derived using iterative algorithms (e.g., a coordiante-descent gratient) such as the ones implemented in the `glmnet` R-package.

#### Fitting Penalized Regressions using the glmnet R-package

The following code shows how to implement Ridge Regression, Lasso, and Elastic Net using the `glmnet` package. By default, `glmnet` fits models over a grid of 100 values of the regularization parameter $\lambda$. The plots produced at the end of the script display, for each of the models, the correlation between predictions and observations in testing data, by value of $\lambda$.

```{r}
 library(glmnet)
 # alpha 0 gives Ridge Regression
 fmRR=glmnet(y=yTRN,x=XTRN,alpha=0)
 dim(fmRR$beta)
 length(fmRR$lambda)
 range(fmRR$lambda)
 
 # alpha 1 gives Lasso
 fmL=glmnet(y=yTRN,x=XTRN,alpha=1)
 
 # alpha between 0 and 1 gives elastic net
 fmEN=glmnet(y=yTRN,x=XTRN, alpha=0.5)
 
 COR.RR=rep(NA,100)
 COR.L=rep(NA,100)
 COR.ENet=rep(NA,100)
 
 # evaluating correlation in TST set
 for(i in 1:100){
   COR.RR[i]=cor(yTST,XTST%*%fmRR$beta[,i])
   COR.L[i]=cor(yTST,XTST%*%fmL$beta[,i])
   COR.ENet[i]=cor(yTST,XTST%*%fmEN$beta[,i])
 }
```


```{r, echo=FALSE,fig.cap="Correlation between predictions and phenotypes in testing, Rdige Regression", out.width = '85%'}
 plot(COR.RR,x=log(fmRR$lambda),type='o',col='blue',cex=.5)
```

```{r, echo=FALSE,fig.cap="Correlation between predictions and phenotypes in testing, Lasso", out.width = '85%'}
 plot(COR.L,x=fmL$df,type='o',  col='blue',cex=.5,xlab='# of active markers')
```

```{r, echo=FALSE,fig.cap="Correlation between predictions and phenotypes in testing, Elastic Net", out.width = '85%'}
 plot(COR.ENet,x=fmEN$df,type='o',col='blue',cex=.5,xlab='# of active markers')
```


**Remarks**

  - The `glmnet` function fits each of the models over a grid of values of $\lambda$, the rules used to choose those values are described in  [Friedman, Hastie, and Tibshirani (2010)](https://www.jstatsoft.org/article/view/v033i01).
  - The matrix `$beta` has the solutions (estimated effects) obtained for each value of $\lambda$` 
  - After fitting the model we evaluate prediction accuracy by correlating the testing phenotypes `yTST` with predictions (see loop above).
  - In the case of Lasso and Elastic Net the default values of lambda led to an internal maxima, i.e., an internal region with maximum correlation. This is not the case for the Ridge Regression, the grid of values of $\lambda$ used in that case may need to use smaller values of $\lambda$. The following example produce new fits using a user-provided grid of values for the regularization parameter.
  
```{r}
  lambda=c(fmRR$lambda,min(fmRR$lambda)*seq(from=0.9,to=0.01,length=50))
  fmRR=glmnet(y=yTRN,x=XTRN,alpha=0,lambda=lambda)
  
  # evaluating correlation in TST set
  COR.RR=rep(NA,length(fmRR$lambda))
  for(i in 1:length(fmRR$lambda)){
   COR.RR[i]=cor(yTST,XTST%*%fmRR$beta[,i])
  }
```

```{r, echo=FALSE,fig.cap="Correlation between predictions and phenotypes in testing, Ridge Regression", out.width = '85%'}
  plot(COR.RR,x=log(fmRR$lambda),type='o',col='blue',cex=.5)
```
  
  - The three models achieve correlations of about 0.45 (Ridge Regression does slightly better)
  - This is much better than what we obtained selecting markers based on their marginal association (correlation ~0.37, Example 1).
  - Remember that estimates of accuracy, such as the ones discussed above, are point estimates subject to sampling variability; there is sampling variance emerging from the sampling of training and testing data. In the In-class assignment you will be asked to repeat the examples using many training-testing partitions; we will use those results to assess sampling variability on these estimates and also to get a more precise estimate. 
  
  
\newpage

### 4) Bayesian Regressions

In a Bayesian model, estimates are obtained from the posterior distribution of the model uknowns (e.g., regression coefficients). 


Recall that from Baye's rule we have that $p(A|B)=\frac{p(A,B)}{p(B)}=\frac{p(B|A)P(A)}{p(B)}$. Taking $A$ to be the model parameters (e.g., $\beta$ in a regression model), and $B$ to be the data ($y$), we have that

$p(\beta|y)=\frac{p(y|\beta)p(\beta)}{p(y)}$

Above, 

  - $p(\beta|y)$ is the posterior distribution of the parameters given the data (the object we use to summarize knowledge and uncertainty about model parameters), 
  - $p(y|\beta)$ is the conditional distribution of the data given the parameters, the likelihood function when viewed as a function of $\beta$, 
  - $p(\beta)$ is the prior distribution of the model uknowns, the object we use to summarize *prior knowledge*, and
  - $p(y)=\int{ p(y|\beta)p(\beta) \,d\beta}$. 
  
The last object, $p(y)$, is the marginal distribution of the data. This object does not involve the uknown parmaeters; therefore, the postrior distribution is proportional to the product of the likelihood times the prior distribution


$p(\beta|y)\propto p(y|\beta)p(\beta)$

This makes evident how the posterior distribution (and inferences from it, e.g., the posterior mean, or the posterior mode) depends on both evidence provided by the data, quantified via the *likelihood function*, and *prior knowledge* summarized by the prior distribution.

\newpage

#### 3.1) A Bayesian model with a Gaussian likelihood and a Gaussian prior

Let's consider a linear model $y=X\beta + \varepsilon$ with a Gaussian likelihood,

$p(y|X,\beta)=N(X\beta,I\sigma^2_\varepsilon)$


The ML estimator can be shown to be the OLS estimator

$\hat{\beta}=(X'X)^{-1}X'y$

Conisder now using a Gaussian IID prior with zero-mean and variance $\sigma^2_{\beta}$, that is

$p(\beta)=N(0,I\sigma^2_{\beta})$

The posterior distribution becomes

$p(\beta|y,\sigma^2_\varepsilon,\sigma^2_{\beta})\propto N(X\beta,I\sigma^2_\varepsilon)\times N(0,I\sigma^2_{\beta})$

This can be shown to be proportional to a Multivariate Normal distribution with mean $\tilde{\beta}=(X'X+I\lambda)^{-1}X'y$ and variance-covariance matrix $V=(X'X+I\lambda)^{-1}\sigma^2$, where $\lambda=\sigma^2/\sigma^2_{\beta}$. Note that $\tilde{\beta}=(X'X+I\lambda)^{-1}X'y$ is the Ridge-regression estimator. Thus, Ridge Regression estiamtes can be seen as the posterior mean (also the posterior mode) of the vector of effects in a Gaussian regression model with IID Gaussian prior.

The regularization parameter, $\lambda=\sigma^2/\sigma^2_{\beta}$, is a noise-to-signal ratio. In penalize regressions, this parameter is often chosen using cross-validation (see section on penalized regressions, above). In a Bayesian context, the variances can be treated as unknown (e.g., by assigning them a scaled-inverse chi-square prior); thus inferring effects and variances jointly from the training data. This is illustrated in the following example wich uses the [BGLR R-package](https://cran.r-project.org/web/packages/BGLR/index.htm).

\newpage 

```{r}
 library(BGLR)
  nIter=6000 # I set this to small value that way it will run quickly, for more serious analyses use longer chains
  burnIn=1000 # and longer burnin
 # Gaussian prior ("Bayesian Ridge-Regression")
  LP=list( list(X=XTRN,model='BRR') ) # 2-level list, allows specifying different types of random and fixed effects
  fmBRR=BGLR(y=yTRN,ETA=LP,nIter=nIter,burnIn=burnIn,saveAt='BRR_',verbose=FALSE)
  # Retriving samples from the variance parameters
  
  vE=scan('BRR_varE.dat',quiet=TRUE)
  vB=scan('BRR_ETA_1_varB.dat',quiet=TRUE)
  lambda=vE/vB
```

Trace plots (left) are used to assess convergence to the posterior distribution; density plots (right) are used to summarize knowledge and uncertainty about parameters given the data

```{r, echo=FALSE,fig.cap="Trace density plots of the error variance.", out.width = '85%'}
  par(mfrow=c(1,2))
  # Trace plot
  plot(vE,type='o',cex=.5,col=2,main='Trace Plot',ylab='Error variance')
  plot(density(vE),xlab='Error variance',main='Density plot')
```

```{r, echo=FALSE,fig.cap="Trace and density plots of the variance of effects.", out.width = '85%'}
  par(mfrow=c(1,2))
  # Trace plot
  plot(vB,type='o',cex=.5,col=2,main='Trace Plot',ylab='Variance of effects')
  plot(density(vB),xlab='Variance of effects',main='Density plot')
```



```{r}
  ## Prediction accuracy in the testing set
  cor( yTST, XTST%*%fmBRR$ETA[[1]]$b)
  max(COR.RR)
```

While the RR appears to outperform slightly the Bayesian model in prediction accuracy, the estimate of prediction accuracy for the RR is likely upwardly biased because, below, in the comparison we choose lambda based on the same testing data that is used to evalaute accuracy. On the other hand, predictions from the Bayesian model were derived using the training data only.

###  3.2) Shrinkage and variable selection priors

The Gaussian prior used in the previous section induces shrinkage without performing any variable selection. In the last decade a pletora of Bayesian models using different priors have been developed. These priors can be classified in three main groups: (i) Gaussian, (ii) Thick-tailed priors, this group includes the double-exponential (used in the [Bayesian Lasso](https://www.tandfonline.com/doi/abs/10.1198/016214508000000337)) and the sclaed-t prior, and (ii) finite mixture priors with a point of mass at zero, these models perform both variable selection and srhinkage. The following figure displays these priors.

\newpage

```{r , echo=FALSE, fig.cap="Prior distributions of effects commonly used in Bayesian models", out.width = '85%'}
 knitr::include_graphics("~/Dropbox/STAT_COMP/2020/priors.png")
```

These priors are scaled by multiple *hyper-parameters* (e.g., degree of freedom, scale, prior probability of non-null effects). Fortunately, some of these parameters can be treated as uknown and can be inferred by data.

The  [BGLR R-package](https://cran.r-project.org/web/packages/BGLR/index.htm) implements some of these priors. The following example illustrates how to fit Bayesian models with different prior distributions of effects. For additional examples and a description of the methods implemented you can check the following [GitHub repository](https://github.com/gdlc/BGLR-R)  (include multiple examples) and the following [manuscript](http://www.genetics.org/content/198/2/483).

**Model fitting**

```{r} 
    
  # Scaled-t
   LP[[1]]$model='BayesA'
   fmBA=BGLR(y=yTRN,ETA=LP,nIter=nIter,burnIn=burnIn,saveAt='BA_',verbose=FALSE)
   
  # Double-Exponential
   LP[[1]]$model='BL'
   fmBL=BGLR(y=yTRN,ETA=LP,nIter=nIter,burnIn=burnIn,saveAt='BL_',verbose=FALSE)
   
  # Spike-slab (Gaussian)
   LP[[1]]$model='BayesC'
   fmBC=BGLR(y=yTRN,ETA=LP,nIter=nIter,burnIn=burnIn,saveAt='BC_',verbose=FALSE)
   
  # Spike-slab (Scaled-t)
   LP[[1]]$model='BayesB'
   fmBB=BGLR(y=yTRN,ETA=LP,nIter=nIter,burnIn=burnIn,saveAt='BB_',verbose=FALSE)
```


**Evaluation of Prediction Accuracy**


```{r}
  bayes=c(
     'BRR'   =cor( yTST, XTST%*%fmBRR$ETA[[1]]$b),
     'BL'    =cor( yTST, XTST%*%fmBL$ETA[[1]]$b),
     'BayesA'=cor( yTST, XTST%*%fmBA$ETA[[1]]$b),
     'BayesB'=cor( yTST, XTST%*%fmBB$ETA[[1]]$b),
     'BayesC'=cor( yTST, XTST%*%fmBC$ETA[[1]]$b)
  )   
  round(bayes,3)
  
```

In general we see no big difference in prediction accuracy, all the Bayesian models achieved in this example a prediction correlation close than the one achieved by penalized regressions.

These models offer more than just predictions. This code illustrates how to extract other parameters (for more details follow the links provided above).


**Retrieving samples and estimates**



```{r,echo=FALSE }
  # Items available to every model
   head(fmBRR$yHat) # predictions (if they were NAs there will be predictions for those points as well)
   fmBRR$mu # intercept (always included by defaul)
   fmBRR$varE # error variance
   fmBRR$fit # DIC and other statistics
   fmBRR$nIter #... other run parameters...
   varE=scan('BRR_varE.dat',quiet=TRUE) # samples from the posterior distribution of the error variance
   varB=scan('BRR_ETA_1_varB.dat',quiet=TRUE) # variance of effects
```

**Estimates (posterior means of effects) and posterior standard deviation of effects**

```{r}
  # Gaussian prior 
   head(fmBRR$ETA[[1]]$b)  # posterior means of effects
   head(fmBRR$ETA[[1]]$SD.b) # posterior SDs
  # Bayes A
    head(fmBA$ETA[[1]]$b)
    head(fmBA$ETA[[1]]$SD.b) # posterior SDs
    
  # Bayesian Lasso
    head(fmBL$ETA[[1]]$b)
    head(fmBL$ETA[[1]]$SD.b) # posterior SDs
    
  # BayesC
    head(fmBC$ETA[[1]]$b)
    head(fmBC$ETA[[1]]$SD.b) # posterior SDs
  # BayesB
    head(fmBB$ETA[[1]]$b)
    head(fmBB$ETA[[1]]$SD.b) # posterior SDs
```

\newpage

**Posterior probability of non-zero effect**

Models `BayesB` and `BayesC` use priors with a point of mass at zero. In these models, at each iteration of the sampler, only a fraction of the predictors have non-zero effects. We can use these models to estimate the overall proportion of non-zero effects, and also the posterior probability of inclusion for each of the predictors.

*Overall proportion of non-zero effects*

```{r}
 fmBC$ETA[[1]]$probIn
 fmBB$ETA[[1]]$probIn
```

*Probability of inclusion by predictor*

```{r}
  plot(fmBC$ETA[[1]]$d) # posterior probability of inclussion
  plot(fmBB$ETA[[1]]$d) # posterior probability of inclussion
```

```{r, echo=FALSE}
 unlink('*.dat')
```