---
title: "Inclass 5 Supplemental"
author: "Andriana Manousidaki"
date: "2023-09-23"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Aim

With this assignment we will practice more on linear regression.
Let us assume that the matrix $X$ has the data values of the predictor variables, $X_1,X_2,...,X_p$ and the matrix $Y$ is the vector of observed data of the response variable.

We aim to find the Ordinary Least Squares Estimator, betaOLS, of the coefficients $ beta = ( beta_1, beta_2,...,beta_p)^T$ of the model $Y = beta_0 + beta_1X_1+ beta_2 X_2 + ...+ beta_pX_p + e$, assuming that:

- $E(e)=0$
- $Cov(e)= s^2_{e}I_n$

When we have small sample size (n< 30) and want to do hypothesis testing on the coefficients, we will add the assumption
- $ e $ follows $N(0,s^2_e I_n)$

# In-class practice

## Read the data

The Boston data set, which records medv (median house value) for 506 census tracts(suburbs) in Boston. 

We will seek to predict medv using 12 predictors such as rm (average number of rooms per house), age (proportion of owner-occupied units built prior to 1940), and lstat (percent of households with low socioeconomic status). For full description see [here](https://rdrr.io/cran/ISLR2/man/Boston.html).

```{r Data}
#install.packages('ISLR2')
library(ISLR2)
head(Boston)

```

## Summary of variables

### Boxplots
```{r}
par(mfrow=c(2,2))
for (i in 1:3) {
boxplot(Boston[,i],main=colnames(Boston)[i],xlab=colnames(Boston)[i],data=Boston)
}

par(mfrow=c(2,2))
for (i in 5:8) {
boxplot(Boston[,i],main=colnames(Boston)[i],xlab=colnames(Boston)[i],data=Boston)
}

par(mfrow=c(2,2))
for (i in 9:12) {
boxplot(Boston[,i],main=colnames(Boston)[i],xlab=colnames(Boston)[i],data=Boston)
}
```

### Scatterplot

```{r}

for (i in 1:11) {
  if (i!=4) {
    plot(medv~Boston[,i],main=colnames(Boston)[i],xlab=colnames(Boston)[i],data=Boston)
  }
}


```
## Fit the model

```{r}
lm.fit<- lm(medv ~ rm + dis + age + lstat, data = Boston)
summary(lm.fit)
```

## Plots to evaluate residual assumptions

```{r}
plot(lm.fit)
```


## F-test 1: Assuming that the assumptions were true, is at least one of the predictors X1, X2,...,Xp useful in predicting the response? or are all coefficients are zero and there is only the intercept in the model?

<!-- The F-test uses the ratio of two scaled sum of squares: the sum of squares of the model (scaled by the model degrees of freedom) and the residual sum of squares (scaled by the residual degree of freedom). -->

<!-- To implement an F-test, we fit the model under the null and alternative hypotheses, and obtain, from each of these models the corresponding residual sum of squares (say RSS0 from H0 and RSSa from Ha). -->

<!-- The amount of variance explained by the factor, after acoOunting for the effects in H0 is: MSS = RSS0 − RSSa (the ‘model sum of squares’), the residual sum of squares is simply the RSS under Ha (i.e., RSSa). -->

<!-- The degree of freedom of RSSa equals df2 = n − p − 1, where p is the number of predictors in Ha. On the other hand, (RSS0 − RSSa) has df1 = pa − p0 (i.e., the difference in the number of parameters between Ha and H0 (df1 = 3), 3 in the example considered above. -->

<!-- Under the null hypothesis both (RSS0 − RSSa)/s^2_e and RSSa/s^2_e are independent, and, under H0, those statistics follow chi-square distributions with df equal to df1 and df2, respectively; therefore, the ratio -->
<!-- (RSS0−RSSa)/(pa−p0)/RSSa/(n−pa−1) follows an F-distribution, with df1 = pa−p0 and df2 = n−pa−1. -->

```{r}
n=dim(Boston)[1]
H0 = lm(medv ~ 1, data = Boston)
Ha = lm(medv ~ rm + dis + age + lstat, data = Boston)

RSS0 = sum(residuals(H0)^2)
RSSA = sum(residuals(Ha)^2)

MSS = RSS0 - RSSA

df1 = length(coef(Ha)) - length(coef(H0))
df1
df2 = n-length(coef(Ha))
df2

Fstat = (MSS/df1)/(RSSA/df2)
Fstat

pval= pf(Fstat, df1 =df1, df2=df2, lower.tail = F)

print(c('Ftest' = Fstat, 'df1'= df1, 'df2'=df2, 'pval' = pval ))
```

```{r}
anova(H0,Ha)
```
## Correlation coefficient
$R^2 = 1 - SSE/SST$
```{r}
R2= 1 - RSSA/RSS0

R2

```
## F-test 2: Test if age and dis  are not useful in predicting medv.
```{r}
n=dim(Boston)[1]
H0 = lm(medv ~ rm + lstat, data = Boston)
Ha = lm(medv ~ rm + dis + age + lstat, data = Boston)

RSS0 = sum(residuals(H0)^2)
RSSA = sum(residuals(Ha)^2)

MSS = RSS0 - RSSA

df1 = length(coef(Ha)) - length(coef(H0))
df1
df2 = n-length(coef(Ha))
df2

Fstat = (MSS/df1)/(RSSA/df2)
Fstat

pval= pf(Fstat, df1 =df1, df2=df2, lower.tail = F)

print(c('Ftest' = Fstat, 'df1'= df1, 'df2'=df2, 'pval' = pval ))

anova(H0, Ha)
```


## Adding interactions in a model

It is easy to include interaction terms in a linear model using the lm() function. The syntax lstat:age tells R to include an interaction term between
lstat and age. The syntax lstat * age simultaneously includes lstat, age,
and the interaction term lstat×age as predictors; it is a shorthand for
lstat + age + lstat:age.

```{r}
summary(lm(medv ~ lstat * age, data = Boston))
```

# Submit by Tuesday midnight

We will attempt to predict Sales (child car seat sales) in 400 locations based on a number of predictors.The Carseats data includes qualitative predictors such as Shelveloc, an indicator of the quality of the shelving location—that is, the space within a store in which the car seat is displayed—at each location. The predictor Shelveloc takes on three possible values: Bad, Medium, and Good.

More on Carseats dataset [here](https://rdrr.io/cran/ISLR2/man/Carseats.html)


## Question 1: Explore the data set Carseats, create boxplots for each variable and a scatterplot of sales vs each of the other variable. Which of the variables do you expect to have a linear relationship with sales?
```{r}
head(Carseats)

```

## Question 2: Predict Carseat sales using all other variables


## Question 3: Explore evaluation plots for the above model. What do you observe, are the assumption regarding the model errors satisfied?


## Question 4: Explore evaluation plots for the above model. What do you observe, are the assumption regarding the model errors satisfied?

## Question 5: Assuming that assumptions about the error term hold. Use an Ftest to answer the following question.Is at least one of the predictors X1, X2,...,Xp useful in predicting the response? or are all coefficients are zero and there is only the intercept in the model?

## Question 6: Use the Ftest to test if the variables Population, Education, Urban, US, Age are useful in predicting Sales.


##  Question 7: Fit the model Sales vs the remaining useful variables and adding some interatcions in the model. Write the regression equation with the estimated coefficients and interpret the coefficients of the intereaction terms.


