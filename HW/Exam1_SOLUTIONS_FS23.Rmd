---
title: 'Statistical Computing: Solutions to Midterm Exam'
author:
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
**Name & email:**

**Instructions:**

1.The exam can be found in a pdf and Rmarkdown form in Github.

2.Use the Rmarkdown form to answer the questions of the exam, using R.

3.You will need to submit your Rmarkdown file and the knit PDF file as your solutions to this exam, in D2L->Assignments-> Midterm Exam.

4.The exam is prepared so that you can complete it by 4:30pm. Notice that you will have until 5 pm to upload your submission on D2L.

5.There are 3 questions in the exam, each in a different page. Each question has sub-questions. The exam is with open notes. Points for each subquestion can be found in a parenthesis.

## Question 1: Maximum Likelihood Estimation for the parameters $\alpha, \beta$ of the Gamma distribution.

Let $X$ be a random variable that follows the Gamma($\alpha, \beta$), where $\alpha$ is the shape parameter and $\beta$ is the rate parameter. Then the density function of $X$ is:

$f(X=x| \alpha, \beta) = \frac{\beta^{\alpha}}{\Gamma(\alpha)} x^{\alpha-1}e^{-\beta x}$.

If $X_1, X_2, X_3,..., X_n$ are n independent and identically distributed random variables that follow the same $Gamma(\alpha, \beta)$,then the likelihood function of $X_1=x_1, X_2=x_2, X_3=x_3,..., X_n=x_n$ is:

$L(\alpha,\beta | x_1,x_2, x_3,..., x_n) = \prod \limits_{i=1}^n \frac{\beta^{\alpha}}{\Gamma(\alpha)} x_i^{\alpha-1}e^{-\beta x_i}$ and the loglikelihood is

$\ell(\alpha,\beta | x_1,x_2, x_3,..., x_n) = \sum \limits_{i=1}^n log(\frac{\beta^{\alpha}}{\Gamma(\alpha)} x_i^{\alpha-1}e^{-\beta x_i}) = nlog(\frac{\beta^{\alpha}}{\Gamma(\alpha)}) + (\alpha -1)\sum \limits_{i=1}^nlog(x_i)-\beta \sum \limits_{i=1}^n x_i.$


**In R:**

-You can use the R function gamma(x) to calculate values of $\Gamma(x)$.

-For your convenience you can find the R functions for the Gamma distribution [here](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/GammaDist.html).

-Notice that for this problem we define a Gamma distribution only by the shape and rate parameter and we ignore the scale parameter.


**1.1)**(18 points) Write a function that evaluates the negative log-likelihood of a random sample of IID Gamma($\alpha, \beta$) distributed random variables.

```{r}
#METHOD 1
negLogLik=function(x,theta){
-sum( dgamma(x,shape=theta[1],rate=theta[2],log=TRUE))
}

# OR

#METHOD 2
negLogLik2=function(x,theta){
n = length(x)
l = n*theta[1]*log(theta[2]) - n*log(gamma(theta[1])) + (theta[1]-1)*sum(log(x)) - theta[2]*sum(x)
return(-l)
}
```

**1.2)** (18 points)Using your function from 1.1) and the following data find the maximum likelihood estimators of $\alpha$ and $\beta$.

**-** Use $\alpha = 1,\beta = 1$ as starting parameters. 

**-** Report your parameter estimates.

```{r, eval=TRUE}
x = as.matrix(read.table('https://raw.githubusercontent.com/gdlc/STAT_COMP/master/DATA/Gamma_data.txt'))
head(x)
```

```{r}
fm1=optim(fn=negLogLik,x=x,par=c(1,1),hessian = TRUE)
fm1
```

OR 

```{r}
fm2=optim(fn=negLogLik2,x=x,par=c(1,1), hessian = TRUE)
fm2
```

Based on the maximization of the log likelihood the estimated $\alpha =$ 3.015348 and the estimated $\beta =$ 2.064274.Notice that the function optimize() can not be used here, because it is used for the maximization of a function with respect to only one parameter. Here we maximize the loglikehood with respect to two parameters.

**1.3)** (18 points)Provide Standard Errors, and 95% Confidence Intervals for each of the parameters.

For the standards errors we use the inverse of the Fisher information matrix, which is the inverse of the hessian of the likelihood computed at the MLE estimators.

```{r}
VAR = solve(fm1$hessian)
SE = sqrt(diag(VAR))
SE
```
```{r}
VAR2 = solve(fm2$hessian)
SE2 = sqrt(diag(VAR2))
SE2
```

#A 95% CI for $\alpha$ will be  

```{r}
lower = fm1$par[1] - 1.96*SE[1]
upper = fm1$par[1] + 1.96*SE[1]

lower
upper
```

#and for $\beta$ 

```{r}
lower = fm1$par[2] - 1.96*SE[2]
upper = fm1$par[2] + 1.96*SE[2]

lower
upper
```

The 95% CI for $\alpha$ is (2.660291, 3.370405) and for $\beta$ is (1.799812, 2.328735).

\newpage
## Question 2: Logistic regression

Below you are provided with a simulated data set containing information on ten thousand customers. Specifically, for each customer we are given information about the following variables:

1.*default* : A factor with levels No and Yes indicating whether the customer defaulted on their debt. Default usually happens after six months in a row of not making at least the minimum payment due.
 
2.*student* : A factor with levels No and Yes indicating whether the customer is a student

3.*balance*: The average balance that the customer has remaining on their credit card after making their monthly payment

4.*income* : Income of customer


```{r, eval = TRUE}
library(ISLR2)
Default_data = Default
head(Default_data)
```

**2.1)** (18 points)We model the probability of defaulting based on the variables *student*, *balance* and *income*. Using the logistic regression model default~student + balance + income, estimate the probability that a student with median balance and median income will default. To do that follow the steps below:

**-**Fit a logistic regression model and

**-**Find the median of the variable balance and the median of the variable income.

**-**Find the predicted probability of defaulting for a student with balance equal with the median balance of the data and income equal with the median income of the data.

**-**Construct 95% Confidence interval for the prediction of the probability of defaulting for a student with median balance and median income.

**-**Based on your results would you expect a student with median income and balance to default?


```{r}
Default_data$default <- ifelse(Default_data$default == 'Yes', 1, 0)
head(Default_data)

fm = glm(default ~ student + balance + income, data = Default_data, family = 'binomial')
summary(fm)

md_balance = median(Default_data$balance)
md_income = median(Default_data$income)

newData = data.frame(student = 'Yes',balance=md_balance, income = md_income)

pred_prob = predict(fm,type = 'response', se.fit = TRUE, newdata = newData)
pred_prob

CI = c(pred_prob$fit-1.96*pred_prob$se.fit,pred_prob$fit+1.96*pred_prob$se.fit)
CI
```

Conclusion, we would not expect a student with median balance and median income to NOT default because:

1. the predicted probability of defaulting for such a student is 0.001246449, which is very low,

2. and based on the produced confidence interval, we are 95% confident that the probability of defaulting will be within the range 0.00057 and 0.0019, which includes very small probabilities of defaulting.

If the predicted probability was close to 0.5 or larger, or if the confidence interval included 0.5, then there would be a chance that a student with median balance and median income could default.

\newpage
## Question 3: Linear regression 

A statistics professor regularly wears a smart watch to keep track of their steps and calories burnt within a day. The smart watch records not only the number of steps taken each day, but also the number of minutes walked at a moderate pace, and the number of miles total that they walked. 

The professor created a data set of the above information for each day and added the variable Rain to record whether it was a rainy, sunny, or cold day.

The following dataset has 68 observations on the following 8 variables.

1.*Steps*:	Total number of steps for the day

2.*Moderate*:	Number of steps at a moderate walking speed

3.*Min*:	Number of minutes walking at a moderate speed

4.*kcal*:	Number of calories burned walking at a moderate speed

5.*Mile*:	Total number of miles walked

6.*Rain*:	Type of weather (rain or shine)

7.*Day*:	Day of the week (U=Sunday, M=Monday, T=Tuesday, W=Wednesday, R=Thursday, F=Friday, S=Saturday

8.*DayType*:	Coded as Weekday or Weekend


```{r}

path='https://vincentarelbundock.github.io/Rdatasets/csv/Stat2Data/Pedometer.csv'

walk_data = read.csv(path, row.names = 1)
```

## Solutions are written based on a significance level of 0.001. If you used a different level, grading was adjusted to your significance level.

**3.1)** (10 points)Use the *walk_data* to fit linear regression model that predicts calories *kcal* burnt on a day from walking as function of *Steps* and *Min*. Summarize in no more than 2 sentence your conclusions regarding the fit of this model.


```{r}
walk_model = lm(kcal~Steps + Min, data = walk_data)

summary(walk_model)

```

We observe that both Min and Steps are useful in predicting calories burnt, since the F-test, reported in the summary, for H0: kcal~1 and Ha: kcal~ Steps+Min, has a small p-value, less than 0.01.

In addition, the parametric test for the coefficients of Steps and the parametric test for the coefficients of Mins have also small p-values (less than 0.01), implying that coefficients are non-zero.

Finally, the adjusted R squared and R squared are close to 1 meaning that there is a good fit and a big percentage of the kcal data can be explained/described by the model.

**3.2)** (18 points)The professor would typically bike on sunny days, so they would walk less on those days. At the same time, if it was raining they would choose to walk instead of biking. Based on that, the variable *Rain* could possibly affect the variables *Steps* and the variable *kcal*.

**-**Test if adding the variable *Rain* and the interaction of *Rain* with *Steps* to the above model, would help the prediction of *kcal*. What is your conclusion?

```{r}
walk_modelf = lm(kcal~Steps + Min + Rain + Rain*Steps, data = walk_data)

anova(walk_model, walk_modelf)
```

The p-value of the F-test, H0: kcal ~ Steps + Min vs Ha: kcal ~ Steps + Min + Rain + Rain * Steps, has a p-value= 0.004534 < 0.01. 

So, we have significant evidence against the model which only includes the variables Min and Step. Including the interaction term is helpful in predicting calories burnt.



